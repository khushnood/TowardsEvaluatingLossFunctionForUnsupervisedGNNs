## Overview

This project focuses on building **pretrained, unsupervised Graph Neural Networks (GNNs)** to enable more generalizable and transferable graph representations. The goal is to design and evaluate models that can learn meaningful graph embeddings without labeled data, and can be fine-tuned for various downstream tasks. We have focussed on emprically finding which loss function and GNN archetcure are best for different downstream tasks.

The source code corresponding to the following paper will be uploaded soon:

**Citation:**
Abbas, Khushnood, Ruizhe Hou, Wengang Zhou, Dong Shi, Niu Ling, Satyaki Nan, and Alireza Abbasi.
[*Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization*](https://www.arxiv.org/abs/2506.14114). arXiv preprint arXiv:2506.14114, 2025.

Stay tuned for updates and implementations related to the proposed unsupervised GNN pretraining framework.
